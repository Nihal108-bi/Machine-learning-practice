{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 50.Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q What is the difference between Series & Dataframes .\n",
    "# Ans:\n",
    ">>A Series and a DataFrame are both data structures in the pandas library, but they serve different purposes:\n",
    "\n",
    ">>Series: A one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). It is similar to a column in a table or a single column in a DataFrame.\n",
    "\n",
    ">>DataFrame: A two-dimensional labeled data structure with columns of potentially different types. It is similar to a table in a database or an Excel spreadsheet. A DataFrame is essentially a collection of Series objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series:\n",
      " 0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: Number, dtype: int64\n",
      "\n",
      "Dataframe:\n",
      "    A  B\n",
      "r  1  6\n",
      "t  2  7\n",
      "p  3  8\n",
      "u  4  9\n"
     ]
    }
   ],
   "source": [
    "#code\n",
    "import pandas  as pd\n",
    "series=pd.Series([1,2,3,4,5],name='Number')\n",
    "print(\"Series:\\n\",series)\n",
    "\n",
    "dataframe=pd.DataFrame({'A':[1,2,3,4],'B':[6,7,8,9]},index=('r','t','p','u'))\n",
    "print(\"\\nDataframe:\\n\",dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:\n",
    "''' \n",
    "Create a database name Travel_Planner in mysql ,and  create a table name bookings in that which having \n",
    "attributes (user_id INT,  flight_id INT,hotel_id INT, activity_id INT,booking_date DATE) .fill with some dummy \n",
    "value .Now you have to read the content of this table using pandas as dataframe.Show the output.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question :Difference between loc and iloc.\n",
    "# Answer:\n",
    "\n",
    "<<loc and iloc are both used for indexing and selecting data in pandas, but they have different approaches:>>\n",
    "\n",
    "<loc:\n",
    "\n",
    "1. Usage: Accesses data by label or boolean array.\n",
    "2. Indexing: Uses row and column labels to select data.\n",
    "3. Slicing: Includes both the start and end labels in slices.\n",
    "\n",
    "<iloc:\n",
    "\n",
    "1. Usage: Accesses data by integer position (index-based).\n",
    "2. Indexing: Uses integer positions to select data.\n",
    "3. Slicing: Excludes the end position in slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "r  1  6\n",
      "t  2  7\n",
      "p  3  8\n",
      "u  4  9 \n",
      "\n",
      "Row 'r':\n",
      "A    1\n",
      "B    6\n",
      "Name: r, dtype: int64\n",
      "\n",
      "Row 't':\n",
      "A    2\n",
      "B    7\n",
      "Name: t, dtype: int64\n",
      "\n",
      "Value at row 'u', column 'A':\n",
      "3\n",
      "\n",
      "Slice from 'r' to 'p':\n",
      "   A  B\n",
      "r  1  6\n",
      "t  2  7\n",
      "p  3  8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'A':[1,2,3,4],'B':[6,7,8,9]},index=('r','t','p','u'))\n",
    "print(df,\"\\n\")\n",
    "\n",
    "#use-case for loc\n",
    "#Access the row lable\n",
    "print(\"Row 'r':\")\n",
    "print(df.loc['r'])\n",
    "\n",
    "print(\"\\nRow 't':\")\n",
    "print(df.loc['t'])\n",
    "\n",
    "#Acces by row and column label\n",
    "print(\"\\nValue at row 'u', column 'A':\")\n",
    "print(df.loc['p','A'])\n",
    "#slice using labels\n",
    "print(\"\\nSlice from 'r' to 'p':\")\n",
    "print(df.loc['r':'p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "r  1  6\n",
      "t  2  7\n",
      "p  3  8\n",
      "u  4  9 \n",
      "\n",
      "Row at index 0:\n",
      "A    1\n",
      "B    6\n",
      "Name: r, dtype: int64\n",
      "\n",
      "Row at index 1:\n",
      "A    2\n",
      "B    7\n",
      "Name: t, dtype: int64\n",
      "\n",
      "Value at row index 3, column index 0:\n",
      "4\n",
      "\n",
      "Slice from index 0 to 2 (exclusive of 2):\n",
      "   A  B\n",
      "r  1  6\n",
      "t  2  7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [6, 7, 8, 9]\n",
    "}, index=['r', 't', 'p', 'u'])\n",
    "print(df,\"\\n\")\n",
    "# Use-case for iloc\n",
    "\n",
    "# Access the row by integer index\n",
    "print(\"Row at index 0:\")\n",
    "print(df.iloc[0])  # Should return the first row (index 0)\n",
    "\n",
    "print(\"\\nRow at index 1:\")\n",
    "print(df.iloc[1])  # Should return the second row (index 1)\n",
    "\n",
    "# Access by row and column index\n",
    "print(\"\\nValue at row index 3, column index 0:\")\n",
    "print(df.iloc[3, 0])  # Should return the value at row index 3, column index 0\n",
    "\n",
    "# Slice using integer positions\n",
    "print(\"\\nSlice from index 0 to 2 (exclusive of 2):\")\n",
    "print(df.iloc[0:2])  # Includes rows at positions 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question :What is the difference between supervised and unsupervised learning.\n",
    "# Answer:\n",
    "\n",
    "### Supervised Learning\n",
    "- **Definition**: In supervised learning, the algorithm is trained on a labeled dataset, meaning that each training example is paired with an output label.\n",
    "- **Goal**: The goal is to learn a mapping from inputs to outputs, such that the model can predict the output for new, unseen data.\n",
    "- **Applications**:\n",
    "  - **Classification**: Predicting categorical labels.\n",
    "    - **Example**:\n",
    "     **Spam Detection** in emails, where the model classifies emails as \"spam\" or \"not spam\".\n",
    "     **Image Recognition**, such as recognizing digits in handwritten notes (e.g., MNIST dataset).\n",
    "\n",
    "  - **Regression**: Predicting continuous values.\n",
    "    - **Example**: \n",
    "    **House Price Prediction**, where the model predicts the price of a house based on features like size, location, and number of rooms.\n",
    "    **Stock Market Forecasting**, where the model predicts future stock prices based on historical data.\n",
    "\n",
    "### Unsupervised Learning\n",
    "- **Definition**: In unsupervised learning, the algorithm is trained on data without labeled responses. The system tries to learn the patterns and the structure from the data.\n",
    "- **Goal**: The goal is to find hidden patterns or intrinsic structures in the input data.\n",
    "- **Applications**:\n",
    "  - **Clustering**: Grouping similar data points together.\n",
    "    - **Example**: \n",
    "      **Customer Segmentation**, where customers are grouped based on purchasing behavior.\n",
    "      **Grouping News Articles**, where articles are grouped by topics without predefined labels.\n",
    "\n",
    "  - **Association**: Discovering rules that describe large portions of the data.\n",
    "    - **Example**:\n",
    "     **Market Basket Analysis**, where associations between items in a shopping cart are identified to suggest items often bought together.\n",
    "     **Recommendation Systems**, such as those used by e-commerce sites to recommend products based on user behavior.\n",
    "\n",
    "  - **Dimensionality Reduction**: Reducing the number of random variables under consideration.\n",
    "    - **Example**: \n",
    "    **Principal Component Analysis (PCA)**, used to reduce the dimensionality of data for visualization or to improve model performance.\n",
    "    **Image Compression**, where the number of features in an image is reduced while retaining essential information.\n",
    "\n",
    "### Key Differences\n",
    "1. **Labeled vs. Unlabeled Data**:\n",
    "   - **Supervised Learning**: Requires labeled data (input-output pairs).\n",
    "   - **Unsupervised Learning**: Uses unlabeled data (no explicit output).\n",
    "\n",
    "2. **Goal**:\n",
    "   - **Supervised Learning**: Predict outcomes for new data based on learned mappings.\n",
    "   - **Unsupervised Learning**: Discover hidden patterns or groupings in data.\n",
    "\n",
    "3. **Common Algorithms**:\n",
    "   - **Supervised Learning**: Linear Regression, Logistic Regression, Decision Trees, Support Vector Machines, Neural Networks.\n",
    "   - **Unsupervised Learning**: K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), Association Rules.\n",
    "\n",
    "4. **Evaluation**:\n",
    "   - **Supervised Learning**: Performance is measured using metrics like accuracy, precision, recall, F1-score, and RMSE based on the labeled data.\n",
    "   - **Unsupervised Learning**: Performance is harder to evaluate, often using metrics like silhouette score, Davies-Bouldin index, and visual inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Explain the bias-variance tradeoff.\n",
    "# Answer :\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that addresses the tradeoff between two sources of error that affect the performance of predictive models:\n",
    "\n",
    "1. **Bias**:\n",
    "   - **Definition**: Bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model.\n",
    "   - **High Bias**: A model with high bias pays very little attention to the training data and oversimplifies the model. This typically leads to **underfitting**, where the model cannot capture the underlying trend of the data.\n",
    "   - **Example**: A linear regression model applied to a dataset that has a nonlinear relationship between features and target values will likely exhibit high bias.\n",
    "\n",
    "2. **Variance**:\n",
    "   - **Definition**: Variance is the error introduced by the model’s sensitivity to small fluctuations in the training data.\n",
    "   - **High Variance**: A model with high variance pays too much attention to the training data, including noise and outliers, and captures these idiosyncrasies. This typically leads to **overfitting**, where the model performs well on training data but poorly on new, unseen data.\n",
    "   - **Example**: A decision tree that is very deep and has many branches might perfectly fit the training data, capturing noise and outliers, but will likely perform poorly on test data.\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "- **Goal**: The goal is to find a balance between bias and variance such that the model generalizes well to new data.\n",
    "- **Tradeoff**:\n",
    "  - **High Bias + Low Variance**: The model is too simple (underfitting). It does not capture the complexity of the data.\n",
    "  - **Low Bias + High Variance**: The model is too complex (overfitting). It captures noise and fluctuations in the training data.\n",
    "  - **Optimal Balance**: A model with low bias and low variance, which captures the underlying data patterns without fitting noise, is ideal.\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "- **Underfitting (High Bias)**: Both training error and test error are high. The model is too simple to capture the data's complexity.\n",
    "- **Overfitting (High Variance)**: Training error is low, but test error is high. The model fits the training data too closely and fails to generalize.\n",
    "- **Just Right**: Both training error and test error are low, indicating that the model generalizes well.\n",
    "\n",
    "### Strategies to Manage the Bias-Variance Tradeoff\n",
    "- **Cross-Validation**: Use cross-validation techniques to assess model performance on different subsets of the data to ensure it generalizes well.\n",
    "- **Regularization**: Apply techniques like L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients and prevent overfitting.\n",
    "- **Ensemble Methods**: Use techniques like bagging (e.g., Random Forest) or boosting (e.g., Gradient Boosting) to reduce variance by averaging multiple models.\n",
    "- **Model Complexity**: Choose an appropriate model complexity that fits the data well but does not overfit. For example, control the depth of decision trees, the number of features in linear models, or the architecture of neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What are precision and recall? How are they different from accuracy.\n",
    "#Answer:\n",
    "Precision, recall, and accuracy are key metrics used to evaluate the performance of classification models. Each metric provides different insights into how well the model performs, especially in the context of imbalanced datasets. \n",
    "\n",
    "### Precision\n",
    "- **Definition**: Precision is the ratio of true positive predictions to the total number of positive predictions made by the model.\n",
    "- **Formula**: \n",
    "  \n",
    "  Precision = True Positives/(True Positives +False Positives)\n",
    "  \n",
    "- **Use Case**: Precision is particularly important in situations where the cost of false positives is high, such as spam detection, where you want to minimize the number of legitimate emails marked as spam.\n",
    "\n",
    "### Recall\n",
    "- **Definition**: Recall (also known as sensitivity or true positive rate) is the ratio of true positive predictions to the total number of actual positive instances.\n",
    "- **Formula**: \n",
    "\n",
    "  Recall = True Positives/(True Positives +False Negatives)\n",
    "\n",
    "- **Use Case**: Recall is crucial in situations where missing positive instances is costly, such as in medical diagnosis, where failing to identify a disease could have serious consequences.\n",
    "\n",
    "### Accuracy\n",
    "- **Definition**: Accuracy is the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances.\n",
    "- **Formula**: \n",
    "\n",
    "Accurancy = (True Positives + True Negatives )/Total Instance\n",
    "\n",
    "- **Use Case**: Accuracy is a useful metric when the classes are balanced, but it can be misleading for imbalanced datasets. For example, in a dataset with 95% negatives and 5% positives, a model that always predicts negative will have high accuracy but poor precision and recall.\n",
    "\n",
    "\n",
    "### Summary\n",
    "- **Precision** measures the correctness of positive predictions.\n",
    "- **Recall** measures the ability to capture actual positive instances.\n",
    "- **Accuracy** measures the overall correctness of the model.\n",
    "l."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:  What is overfitting and how can it be prevented?\n",
    "# Answer:\n",
    "**Overfitting** is when a machine learning model learns the details and noise in the training data to the extent that it performs well on training data but poorly on new, unseen data. It essentially captures random fluctuations in the training data rather than the underlying distribution.\n",
    "\n",
    "**Prevention Strategies**:\n",
    "1. **Cross-Validation**: Use techniques like k-fold cross-validation to ensure the model generalizes well to unseen data.\n",
    "2. **Regularization**: Apply regularization methods (L1, L2) to penalize complex models.\n",
    "3. **Pruning**: In decision trees, remove branches that have little importance.\n",
    "4. **Simplify the Model**: Choose a less complex model that has fewer parameters.\n",
    "5. **Early Stopping**: In iterative algorithms like gradient descent, stop training when performance on a validation set starts to degrade.\n",
    "6. **Ensemble Methods**: Use techniques like bagging and boosting to combine multiple models and reduce overfitting.\n",
    "7. **Increase Training Data**: More data can help the model learn the true underlying patterns rather than noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Explain the concept of cross-validation.\n",
    "# Ans:\n",
    "<<Cross-validation is a technique used to evaluate the performance of a machine learning model and ensure its generalizability to an independent dataset. It involves partitioning the data into subsets, training the model on some subsets, and validating it on the remaining subsets. This process helps in assessing how the model will perform on unseen data.\n",
    "\n",
    ">>Steps in Cross-Validation:\n",
    "\n",
    "1. Split the Data: Divide the dataset into k equal-sized subsets or folds.\n",
    "\n",
    "2. Training and Validation: \n",
    "  >>For each fold, use k-1 folds for training the model.\n",
    "  >>Use the remaining fold for validating the model.\n",
    "\n",
    "3. Repeat: Repeat the process k times, each time with a different fold as the validation set.\n",
    "\n",
    "4. Aggregate Results: Calculate the performance metric (e.g., accuracy, precision, recall) for each fold and then average these results to get a more reliable estimate of the model's performance.\n",
    "\n",
    "<<<Common Types of Cross-Validation:>>>\n",
    "1<<k-Fold Cross-Validation: The dataset is divided into k folds. The model is trained k times, each time using a different fold as the validation set and the remaining k-1 folds as the training set.\n",
    "\n",
    "2<<Stratified k-Fold Cross-Validation: Similar to k-Fold, but ensures that each fold has a representative proportion of each class, which is particularly useful for imbalanced datasets.\n",
    "\n",
    "3<<Leave-One-Out Cross-Validation (LOOCV): A special case of k-Fold where k is equal to the number of data points. Each data point is used once as a validation set, and the rest as the training set.\n",
    "\n",
    "4<<Holdout Method: The dataset is split into two parts: a training set and a validation set. The model is trained on the training set and validated on the validation set. This is a simpler but less reliable method compared to k-Fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What is the difference between a classification and a regression problem.\n",
    "# Ans:\n",
    "\n",
    ">Classification and regression are two types of supervised learning problems in machine learning. Here are the key differences between them:\n",
    "\n",
    "<<<<Classification>>>>\n",
    "<Definition:\n",
    "Classification is a type of supervised learning where the goal is to predict the categorical class labels of new instances, based on past observations. The output variable is a category or class.\n",
    "\n",
    ">Output: Discrete values (e.g., class labels such as \"spam\" or \"not spam\").\n",
    "<Examples: \n",
    "Email spam detection, \n",
    "image recognition (e.g., identifying whether an image contains a cat or a dog), \n",
    "medical diagnosis (e.g., predicting whether a patient has a disease or not).\n",
    "<<Algorithms: Logistic regression, decision trees, random forests, support vector machines (SVM), neural networks.\n",
    "\n",
    "\n",
    "<<<<Regression>>>>\n",
    "<Definition: Regression is a type of supervised learning where the goal is to predict a continuous numerical value based on past observations. The output variable is a real number.\n",
    ">Output: Continuous values (e.g., predicting a numerical value such as price, temperature, or age).\n",
    "<Examples: \n",
    "Predicting house prices, \n",
    "forecasting stock prices, \n",
    "estimating the amount of rainfall.\n",
    "<<Algorithms: Linear regression, polynomial regression, support vector regression (SVR), decision trees, random forests, neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Explain the concept of ensemble learning.\n",
    "# Ans:\n",
    "Ensemble learning is a machine learning technique that combines multiple models (often called \"weak learners\") to create a more powerful model (often called an \"ensemble\") that improves the overall performance and robustness compared to individual models. The main idea is that by combining the predictions of several models, the ensemble can reduce errors and improve generalization.\n",
    "\n",
    "<<<Key Concepts in Ensemble Learning:\n",
    "1. Weak Learners: These are individual models that may not perform very well on their own. Examples include decision trees, linear classifiers, or any other simple models.\n",
    "2. Strong Learner: The combined model that results from the ensemble of weak learners, which typically performs better than any of the individual weak learners.\n",
    "\n",
    "\n",
    "<<Types of Ensemble Methods:>>\n",
    "\n",
    "<Bagging (Bootstrap Aggregating):\n",
    "Definition: \n",
    "<<Bagging involves training multiple models on different subsets of the training data (created by random sampling with replacement) and then averaging their predictions (for regression) or taking a majority vote (for classification).\n",
    "\n",
    "Example: Random Forest, which is an ensemble of decision trees.\n",
    "\n",
    "<Boosting:\n",
    "Definition: \n",
    "<<Boosting involves training models sequentially, where each model tries to correct the errors of the previous one. The final prediction is a weighted sum of the predictions of all models.\n",
    "\n",
    "Example: AdaBoost, Gradient Boosting Machines (GBM), XGBoost.\n",
    "\n",
    "<Stacking:\n",
    "Definition: \n",
    "<<Stacking involves training multiple models (base learners) and then using another model (meta-learner) to combine their predictions. The meta-learner is trained on the outputs of the base learners.\n",
    "\n",
    "Example: Using logistic regression as a meta-learner to combine the predictions of several base learners like decision trees, SVMs, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What is gradient descent and how does it work.\n",
    "# Ans:\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning and statistical modeling. It is particularly used in training machine learning models, such as linear regression and neural networks, to find the optimal model parameters.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Cost Function**:\n",
    "   - Also known as the loss function, it measures how well the model's predictions match the actual data.\n",
    "   - The goal of gradient descent is to minimize this function.\n",
    "\n",
    "2. **Gradient**:\n",
    "   - The gradient is a vector of partial derivatives of the cost function with respect to the model parameters.\n",
    "   - It points in the direction of the steepest ascent of the cost function.\n",
    "\n",
    "3. **Learning Rate**:\n",
    "   - A hyperparameter that determines the step size at each iteration while moving toward the minimum of the cost function.\n",
    "   - A high learning rate can speed up convergence but may overshoot the minimum, while a low learning rate ensures convergence but might be slow.\n",
    "\n",
    "### How Gradient Descent Works\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the model parameters (e.g., weights) with random values or zeros.\n",
    "\n",
    "2. **Compute the Cost**:\n",
    "   - Calculate the cost function using the current parameters.\n",
    "\n",
    "3. **Compute the Gradient**:\n",
    "   - Compute the gradient of the cost function with respect to each parameter. This involves taking partial derivatives of the cost function.\n",
    "\n",
    "4. **Update the Parameters**:\n",
    "   - Update each parameter by moving in the opposite direction of the gradient.\n",
    "   - The update rule is typically: `parameter = parameter - learning_rate * gradient`.\n",
    "\n",
    "5. **Iterate**:\n",
    "   - Repeat steps 2-4 until the cost function converges to a minimum (i.e., the change in cost function is below a certain threshold) or a predefined number of iterations is reached.\n",
    "\n",
    "### Types of Gradient Descent\n",
    "\n",
    "1. **Batch Gradient Descent**:\n",
    "   - Uses the entire training dataset to compute the gradient at each iteration.\n",
    "   - Pros: Stable convergence.\n",
    "   - Cons: Can be slow and computationally expensive for large datasets.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**:\n",
    "   - Uses a single training example to compute the gradient at each iteration.\n",
    "   - Pros: Faster and can handle large datasets.\n",
    "   - Cons: Can be noisy and may not converge as smoothly.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**:\n",
    "   - Uses a small, randomly selected subset of the training dataset (mini-batch) to compute the gradient at each iteration.\n",
    "   - Pros: Balances the trade-off between batch gradient descent and SGD, providing faster convergence and less noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Describe the difference between batch gradient descent and stochastic gradient descent.\n",
    "# Ans:\n",
    "\n",
    "### Batch Gradient Descent vs. Stochastic Gradient Descent\n",
    "\n",
    "**Batch Gradient Descent**:\n",
    "- **Data Used**: Uses the entire training dataset to compute the gradient of the cost function at each iteration.\n",
    "- **Update Frequency**: Updates model parameters after processing the whole dataset.\n",
    "- **Convergence**: Provides a stable convergence path but may be slow for large datasets.\n",
    "- **Computational Cost**: High, as it requires processing the entire dataset to compute each gradient.\n",
    "- **Pros**: More stable updates, better convergence properties.\n",
    "- **Cons**: Computationally expensive, slow for large datasets.\n",
    ">>>Initialize model\n",
    " << model = LinearRegression()>>\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)**:\n",
    "- **Data Used**: Uses a single training example to compute the gradient of the cost function at each iteration.\n",
    "- **Update Frequency**: Updates model parameters after processing each individual training example.\n",
    "- **Convergence**: Faster but can be noisy with frequent updates, making convergence less stable.\n",
    "- **Computational Cost**: Low, as it processes one data point at a time.\n",
    "- **Pros**: Faster, suitable for large datasets and online learning.\n",
    "- **Cons**: Noisy updates, can oscillate and may converge to a suboptimal solution.\n",
    ">>>Initialize model\n",
    "<<model = SGDRegressor() >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question :What is the curse of dimensionality in machine learning.\n",
    "# Ans:\n",
    "**Curse of Dimensionality**:\n",
    "- **Definition**: The curse of dimensionality refers to the various problems and challenges that arise when working with high-dimensional data. As the number of features (dimensions) increases, the volume of the feature space grows exponentially, making data sparsity a significant issue.\n",
    "- **Issues**:\n",
    "  - **Data Sparsity**: High-dimensional spaces become sparse, leading to insufficient data coverage.\n",
    "  - **Increased Computational Cost**: More dimensions increase the computational complexity and time required for algorithms.\n",
    "  - **Overfitting**: Models may become overly complex and overfit the training data due to the large number of features.\n",
    "  - **Distance Metrics**: Distances between data points become less meaningful, affecting distance-based algorithms like k-NN.\n",
    "\n",
    "**Mitigation Strategies**:\n",
    "1. **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) reduce the number of features while preserving important information.\n",
    "2. **Feature Selection**: Use methods to select the most relevant features and discard irrelevant or redundant ones (e.g., using feature importance scores or statistical tests).\n",
    "3. **Regularization**: Apply regularization techniques (e.g., L1/L2 regularization) to penalize large feature weights and prevent overfitting.\n",
    "4. **Data Augmentation**: Increase the amount of training data to better cover the feature space.\n",
    "5. **Simpler Models**: Use simpler models that are less prone to overfitting in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:  Explain the difference between L1 and L2 regularization.\n",
    "# Ans: \n",
    "\n",
    "<<L1 and L2 regularization** are techniques used to prevent overfitting in machine learning models by adding a penalty to the loss function based on the magnitude of the model parameters. Here’s a concise comparison:\n",
    "\n",
    "<<< **L1 Regularization (Lasso)**\n",
    "\n",
    "- **Penalty Term**: Adds the absolute value of the coefficients to the loss function.\n",
    "\n",
    "- **Effect**:\n",
    "   - **Feature Selection**: Can drive some coefficients to exactly zero, effectively selecting a subset of features and performing feature selection.\n",
    "   - **Sparsity**: Promotes sparsity in the model parameters, leading to simpler models with fewer non-zero coefficients.\n",
    "\n",
    "- **Use Case**: Useful when you suspect that many features are irrelevant or when you want a sparse solution.\n",
    "\n",
    "<<< **L2 Regularization (Ridge)**\n",
    "\n",
    "- **Penalty Term**: Adds the squared value of the coefficients to the loss function.\n",
    "\n",
    "- **Effect**:\n",
    "   - **Shrinkage**: Shrinks the coefficients toward zero but does not set them exactly to zero. All features remain in the model, but their impact is reduced.\n",
    "   - **Smoothness**: Promotes smaller coefficients, making the model more robust and less sensitive to the data.\n",
    "\n",
    "- **Use Case**: Useful when you want to prevent large coefficients and ensure all features are included in the model.\n",
    "\n",
    "### **Comparison**\n",
    "\n",
    "**Sparsity**: L1 regularization can produce sparse models with some coefficients exactly zero, while L2 regularization tends to shrink coefficients but not to zero.\n",
    "\n",
    "**Computation**: L1 regularization can lead to a non-smooth optimization problem, while L2 regularization results in a smooth and differentiable optimization problem.\n",
    "\n",
    "**Feature Selection**: L1 can perform automatic feature selection by zeroing out less important features, whereas L2 regularization retains all features but reduces their impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question :What is a confusion matrix and how is it used.\n",
    "# Answer:\n",
    "\n",
    "**confusion matrix** is a table used to evaluate classification model performance. It compares predicted labels with actual labels.\n",
    "\n",
    "### Components\n",
    "<True Positive (TP)**: Correctly predicted positives.\n",
    ">True Negative (TN)**: Correctly predicted negatives.\n",
    "<False Positive (FP)**: Incorrectly predicted positives.\n",
    ">False Negative (FN)**: Incorrectly predicted negatives.\n",
    "\n",
    "### Metrics\n",
    "<<Accuracy:  (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "<<Precision:  TP/(TP + FP)\n",
    "\n",
    "<<Recall:  TP/(TP + FN)\n",
    "\n",
    "<<F1 Score:  2( Precision X Recall)/(Precision + Recall)\n",
    "\n",
    "### Uses\n",
    "- **Performance Evaluation**: Measures accuracy, precision, recall, and F1 score.\n",
    "- **Error Analysis**: Identifies specific types of prediction errors.\n",
    "- **Class Imbalance**: Helps detect bias toward certain classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question: Define AUC-ROC curve.\n",
    "#Ans:\n",
    "The **AUC-ROC curve** is a performance measurement for classification problems at various threshold settings. It is used to evaluate the ability of a model to distinguish between classes.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **ROC Curve (Receiver Operating Characteristic Curve)**:\n",
    "  - **Definition**: A plot of the True Positive Rate (TPR) against the False Positive Rate (FPR) at different threshold values.\n",
    "  >>True Positive Rate (Recall):  TP/(TP + FN)\n",
    "\n",
    "  >>False Positive Rate:    FP/(FP + TN)\n",
    "\n",
    "- **AUC (Area Under the ROC Curve)**:\n",
    "  - **Definition**: The area under the ROC curve, representing the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance.\n",
    "  - **Range**: 0 to 1.\n",
    "    - **AUC = 1**: Perfect model with no false positives or false negatives.\n",
    "    - **AUC = 0.5**: Model performs no better than random guessing.\n",
    "    - **AUC < 0.5**: Model performs worse than random guessing (may indicate an issue with the model).\n",
    "\n",
    "### Summary\n",
    "- **ROC Curve**: Shows trade-offs between true positive rate and false positive rate across different thresholds.\n",
    "- **AUC**: Quantifies the overall ability of the model to discriminate between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:Explain the k-nearest neighbors algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "**k-Nearest Neighbors (k-NN)** is a simple, instance-based learning algorithm used for classification and regression.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Distance Metric**: Measures similarity (e.g., Euclidean distance).\n",
    "- **k**: Number of nearest neighbors to consider.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Choose k**: Define the number of neighbors.\n",
    "2. **Calculate Distances**: Compute distance from the new point to all other points.\n",
    "3. **Sort and Select**: Find the k closest neighbors.\n",
    "4. **Classify or Predict**:\n",
    "   - **Classification**: Assign the most common class among the k neighbors.\n",
    "   - **Regression**: Predict the average value of the k neighbors.\n",
    "\n",
    "### Example\n",
    "- **k = 3**: For a new point, find the 3 nearest neighbors. If 2 out of 3 neighbors are in Class A, classify the point as Class A.\n",
    "# Initialize KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "- **Pros**:\n",
    "  - Simple and intuitive.\n",
    "  - No training phase.\n",
    "\n",
    "- **Cons**:\n",
    "  - Computationally intensive for large datasets.\n",
    "  - Sensitive to irrelevant features and data scaling.\n",
    "\n",
    "### Summary\n",
    "k-NN classifies or predicts values based on the closest k neighbors. It’s straightforward but can be slow and affected by feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: Explain the basic concept of a Support Vector Machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "A <Support Vector Machine (SVM)> is a supervised learning algorithm used for classification and regression tasks. Its primary goal is to find the optimal hyperplane that separates different classes in the feature space.\n",
    "\n",
    "### Basic Concept\n",
    "\n",
    "1. **Hyperplane**:\n",
    "   - A hyperplane is a decision boundary that separates data points into different classes. In a 2D space, it’s a line; in 3D, it’s a plane; and in higher dimensions, it’s a hyperplane.\n",
    "\n",
    "2. **Margin**:\n",
    "   - The margin is the distance between the hyperplane and the nearest data points from either class. SVM aims to maximize this margin, creating the best possible separation between classes.\n",
    "\n",
    "3. **Support Vectors**:\n",
    "   - Support vectors are the data points closest to the hyperplane. These points define the margin and are crucial for determining the optimal hyperplane.\n",
    "\n",
    "4. **Optimal Hyperplane**:\n",
    "   - The hyperplane that maximizes the margin between the support vectors of different classes is considered optimal. This helps ensure better generalization to unseen data.\n",
    "\n",
    "5. **Kernel Trick**:\n",
    "   - For non-linearly separable data, SVM uses a kernel trick to map the data into a higher-dimensional space where a linear hyperplane can be used. Common kernels include polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: How does the kernel trick work in SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "The **kernel trick** in SVM allows handling non-linearly separable data by implicitly mapping it to a higher-dimensional space where a linear hyperplane can separate classes.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Transformation**: Maps data to a higher-dimensional space where it might be linearly separable.\n",
    "2. **Kernel Function**: Computes the inner product between transformed data points directly in the original space, avoiding explicit transformation. Common kernels include:\n",
    "   >Polynomial Kernel\n",
    "   >RBF (Gaussian) Kernel\n",
    "   >Sigmoid Kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: What are the different types of kernels used in SVM and when would you use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "In Support Vector Machines (SVM), different types of kernels are used to handle various types of data and to capture different patterns. Here’s a summary of the most commonly used kernels and their typical use cases:\n",
    "\n",
    "### 1. **Linear Kernel**\n",
    "- **Formula**:   x_i.x_j \n",
    "- **Description**: Represents a linear decision boundary in the original feature space.\n",
    "- **Use Case**: Suitable when the data is linearly separable or nearly linearly separable. It’s also efficient for high-dimensional data.\n",
    "\n",
    "### 2. **Polynomial Kernel**\n",
    "- **Formula**:  K(x_i, x_j) = (x_i.x_j + c)^d\n",
    "\n",
    "- **Description**: Maps the data into a higher-dimensional space using polynomial functions of degree d.\n",
    "c is a constant that shifts the kernel.\n",
    "\n",
    "- **Use Case**: Useful for capturing interactions between features and for datasets where relationships are polynomial in nature. Choose  d  to control the complexity.\n",
    "\n",
    "### 3. **Radial Basis Function (RBF) or Gaussian Kernel**\n",
    "- **Formula**:   K(x_i, x_j)=exp(-|x_i - x_j|^2 /2σ^2  )\n",
    "\n",
    "- **Description**: Maps data into an infinite-dimensional space and measures similarity based on distance. σ  is a parameter that controls the width of the Gaussian function.\n",
    "\n",
    "- **Use Case**: Effective for capturing complex, non-linear relationships. It’s widely used when the decision boundary is not linearly separable and you don’t know the specific nature of the non-linearity.\n",
    "\n",
    "### 4. **Sigmoid Kernel**\n",
    "- **Formula**: K(x_i, x_j) = tanh(αx_i.x_j + c)\n",
    "\n",
    "- **Description**: Uses a hyperbolic tangent function, similar to an activation function in neural networks. α and  c  are parameters that control the shape of the kernel.\n",
    "\n",
    "- **Use Case**: Often used in neural network applications and when you want to mimic the behavior of neural network layers. Less common in practice compared to RBF and Polynomial kernels.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Linear Kernel**: Use when data is linearly separable or nearly so.\n",
    "- **Polynomial Kernel**: Use for capturing polynomial relationships and interactions between features.\n",
    "- **RBF (Gaussian) Kernel**: Use for complex, non-linear relationships and when you do not know the specific form of non-linearity.\n",
    "- **Sigmoid Kernel**: Use in cases where you want to mimic neural network behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:What is the hyperplane in SVM and how is it determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ans:\n",
    "\n",
    "<<Hyperplane in SVM:>>\n",
    "A hyperplane is a decision boundary that separates different classes in the feature space.\n",
    "In a 2D space, the hyperplane is a line; in a 3D space, it is a plane; and in higher dimensions, it is a hyperplane.\n",
    "\n",
    "<Determination of the Hyperplane:>\n",
    "\n",
    "<Objective:\n",
    "The goal is to find the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the nearest data points from each class, known as support vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Question: What are the pros and cons of using a Support Vector Machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ans:\n",
    "<<Pros:>>\n",
    "\n",
    "1. Effective in High-Dimensional Spaces:\n",
    "<SVMs are particularly effective in cases where the number of dimensions is greater than the number of samples.\n",
    "\n",
    "2. Memory Efficient:\n",
    "<SVMs use a subset of training points (support vectors) in the decision function, making them memory efficient.\n",
    "\n",
    "3. Versatile with Different Kernel Functions:\n",
    "<SVMs can be customized with different kernel functions (linear, polynomial, RBF, etc.) to handle various types of data and decision boundaries.\n",
    "\n",
    "4. Robust to Overfitting:\n",
    "<With the appropriate choice of kernel and regularization parameter, SVMs can be robust to overfitting, especially in high-dimensional space.\n",
    "\n",
    "5. Clear Margin of Separation:\n",
    "<SVMs aim to maximize the margin between classes, which can lead to better generalization.\n",
    "\n",
    "\n",
    "<<Cons:>>\n",
    "1. Computationally Intensive:\n",
    "<Training an SVM can be computationally intensive, especially with large datasets, due to the quadratic programming problem involved.\n",
    "\n",
    "2. Choice of Kernel:\n",
    "<The performance of SVMs heavily depends on the choice of the kernel function and its parameters, which may require extensive experimentation and tuning.\n",
    "\n",
    "3. Not Suitable for Large Datasets:\n",
    "<SVMs can be less efficient on very large datasets, both in terms of training time and memory usage.\n",
    "\n",
    "4.Difficult to Interpret:\n",
    "<The resulting model, especially with non-linear kernels, can be difficult to interpret compared to simpler models like decision trees.\n",
    "\n",
    "5. Sensitive to Noise:\n",
    "<SVMs can be sensitive to noisy data and outliers, which can affect the placement of the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: Explain the difference between a hard margin and a soft margin SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans: \n",
    "### Hard Margin SVM\n",
    "- **Definition**: Requires that all training data points are perfectly separated by the hyperplane with no misclassifications.\n",
    "- **Use Case**: Suitable for linearly separable data with no noise.\n",
    "- **Limitation**: Not effective for data with overlaps or outliers, as it cannot tolerate any errors.\n",
    "\n",
    "### Soft Margin SVM\n",
    "- **Definition**: Allows some misclassifications to achieve a better overall model by introducing slack variables.\n",
    "- **Use Case**: Suitable for non-linearly separable data or data with noise and outliers.\n",
    "- **Advantage**: Balances the trade-off between maximizing the margin and minimizing classification errors, providing better generalization.\n",
    "\n",
    "### Summary\n",
    "- **Hard Margin SVM**: No errors allowed; suitable for clean, linearly separable data.\n",
    "- **Soft Margin SVM**: Allows some errors; suitable for noisy or non-linearly separable data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Questio: Describe the process of constructing a decision tree.\n",
    "#Ans:\n",
    "\n",
    "<<<<Process of Constructing a Decision Tree>>>\n",
    "1. Select the Best Attribute:\n",
    "\n",
    "<Criterion: Use a criterion like Information Gain, Gini Index, or Chi-Square to select the attribute that best splits the data.\n",
    "<Information Gain: Measures the reduction in entropy.\n",
    "<Gini Index: Measures the impurity of a dataset.\n",
    "<Chi-Square: Measures the statistical significance of the attribute.\n",
    "\n",
    "2. Split the Dataset:\n",
    "<Based on the selected attribute, split the dataset into subsets. Each subset should contain data points that have the same value for the selected attribute.\n",
    "\n",
    "3. Create a Decision Node:\n",
    "<Create a decision node that represents the selected attribute. This node will have branches corresponding to the possible values of the attribute.\n",
    "\n",
    "4. Repeat for Each Subset:\n",
    "<For each subset created in step 2, repeat the process:\n",
    " 1. If a subset is pure (i.e., all data points belong to the same class), create a leaf node with the class label.\n",
    " 2.If a subset is not pure, select the best attribute for that subset and split it further.\n",
    "\n",
    "5. Stop Criteria:\n",
    "<The process stops when one of the following conditions is met:\n",
    " 1.All data points in a subset belong to the same class (pure subset).\n",
    " 2.There are no remaining attributes to split on.\n",
    " 3.The maximum depth of the tree is reached (if specified).\n",
    " 4.The subset size is below a predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code exmple\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Initialize Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:Describe the working principle of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "### Working Principle of a Decision Tree\n",
    "\n",
    "A decision tree is a supervised learning algorithm used for classification and regression tasks. Its working principle involves recursively splitting the dataset into subsets based on the value of features to create a tree-like model of decisions.\n",
    "\n",
    "### Key Steps in the Working Principle\n",
    "\n",
    "1. **Starting Point: Root Node**\n",
    "   - The process begins with the entire dataset represented by the root node.\n",
    "\n",
    "2. **Choosing the Best Split**\n",
    "   - At each node, the algorithm evaluates all possible splits for all features.\n",
    "   - The best split is determined based on a criterion that measures the quality of the split:\n",
    "     - **Gini Impurity**: Measures the impurity of the node. Lower Gini impurity indicates a better split.\n",
    "     - **Entropy (Information Gain)**: Measures the reduction in entropy (uncertainty) after the split. Higher information gain indicates a better split.\n",
    "     - **Variance Reduction**: Used for regression trees to measure the reduction in variance after the split.\n",
    "\n",
    "3. **Splitting the Data**\n",
    "   - The dataset is divided into subsets based on the selected split.\n",
    "   - Each subset corresponds to a branch of the node, leading to child nodes.\n",
    "\n",
    "4. **Creating Child Nodes**\n",
    "   - The subsets formed by the split become the child nodes of the current node.\n",
    "   - The process of evaluating and splitting continues recursively for each child node.\n",
    "\n",
    "5. **Recursive Splitting**\n",
    "   - The algorithm continues splitting nodes until a stopping criterion is met:\n",
    "     - All data points in a node belong to the same class (for classification).\n",
    "     - No further information gain can be achieved.\n",
    "     - The maximum depth of the tree is reached.\n",
    "     - The number of data points in a node falls below a minimum threshold.\n",
    "\n",
    "6. **Leaf Nodes**\n",
    "   - Nodes where no further splits can be made are called leaf nodes.\n",
    "   - In classification tasks, leaf nodes represent class labels.\n",
    "   - In regression tasks, leaf nodes represent predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: What is information gain and how is it used in decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "### Information Gain in Decision Trees\n",
    "\n",
    "**Information Gain** is a measure used to determine which feature to split on at each step in the construction of a decision tree. It quantifies the reduction in entropy (uncertainty or impurity) achieved by splitting a dataset based on a particular feature.\n",
    "\n",
    "### How Information Gain is Used in Decision Trees\n",
    "\n",
    "1. **Calculate Entropy of the Entire Dataset**:\n",
    "   - Determine the initial entropy of the dataset before any splits.\n",
    "\n",
    "2. **Evaluate Each Feature**:\n",
    "   - For each feature, calculate the entropy of the dataset after splitting based on that feature.\n",
    "   - Compute the weighted sum of the entropies of the resulting subsets.\n",
    "\n",
    "3. **Compute Information Gain**:\n",
    "   - Subtract the weighted sum of the entropies of the subsets from the initial entropy of the dataset. This gives the information gain for that feature.\n",
    "\n",
    "4. **Choose the Best Feature**:\n",
    "   - The feature with the highest information gain is chosen as the splitting feature for the current node.\n",
    "\n",
    "5. **Repeat the Process**:\n",
    "   - For each child node created by the split, repeat the process of calculating entropy, information gain, and selecting the best feature until stopping criteria are met (e.g., all data points in a node belong to the same class, no further information gain can be achieved, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question: Explain Gini impurity and its role in decision trees.\n",
    "#Ans:\n",
    "### Gini Impurity in Decision Trees\n",
    "\n",
    "**Gini Impurity** is a measure used to determine the quality of a split in decision tree algorithms. It calculates the likelihood of incorrectly classifying a randomly chosen element from the dataset if it were labeled according to the distribution of labels in the subset.\n",
    "\n",
    "### Role of Gini Impurity in Decision Trees\n",
    "\n",
    "1. **Splitting Criterion**:\n",
    "   - Gini impurity is used to evaluate and choose the feature and threshold for splitting the data at each node in the tree.\n",
    "   - The goal is to find the split that minimizes the weighted average Gini impurity of the resulting subsets.\n",
    "\n",
    "2. **Calculation**:\n",
    "   - **Before Split**: Calculate the Gini impurity of the parent node.\n",
    "   - **After Split**: Calculate the Gini impurity for each child node and the weighted average Gini impurity of the split.\n",
    "\n",
    "3. **Choosing the Best Split**:\n",
    "   - For each feature and possible split value, compute the resulting Gini impurity.\n",
    "   - Choose the split that results in the lowest weighted average Gini impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:  What are the advantages and disadvantages of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "### Advantages of Decision Trees\n",
    "\n",
    "1. **Easy to Understand and Interpret**:\n",
    "   - Decision trees are intuitive and easy to visualize, making them accessible to both technical and non-technical stakeholders.\n",
    "\n",
    "2. **Simple and Clear Representation**:\n",
    "   - The logic behind decision trees can be easily understood and the rules can be communicated effectively.\n",
    "\n",
    "3. **Requires Little Data Preparation**:\n",
    "   - No need for feature scaling or normalization. They can handle both numerical and categorical data.\n",
    "\n",
    "4. **Handles Nonlinear Relationships**:\n",
    "   - Decision trees can capture complex nonlinear relationships between features and the target variable.\n",
    "\n",
    "5. **Feature Importance**:\n",
    "   - They provide a clear indication of which features are most important for prediction.\n",
    "\n",
    "6. **Non-parametric**:\n",
    "   - They do not assume any underlying distribution of the data, making them versatile in various applications.\n",
    "\n",
    "### Disadvantages of Decision Trees\n",
    "\n",
    "1. **Prone to Overfitting**:\n",
    "   - Decision trees can create overly complex trees that do not generalize well to unseen data. This can lead to poor performance on new data.\n",
    "\n",
    "2. **Instability**:\n",
    "   - Small changes in the data can result in a completely different tree being generated. This is because the tree is highly sensitive to variations in the data.\n",
    "\n",
    "3. **Bias Toward Dominant Classes**:\n",
    "   - In datasets with imbalanced classes, decision trees can be biased towards the majority class, leading to poor performance on the minority class.\n",
    "\n",
    "4. **Greedy Algorithm**:\n",
    "   - The splitting process is based on a greedy algorithm that may not always lead to the globally optimal decision tree.\n",
    "\n",
    "5. **Lack of Smoothness**:\n",
    "   - Decision boundaries created by decision trees can be very jagged, leading to high variance.\n",
    "\n",
    "6. **Limited Expressiveness**:\n",
    "   - For complex relationships and interactions, decision trees may not capture the intricacies of the data as effectively as other methods.\n",
    "\n",
    "### Mitigating Disadvantages\n",
    "\n",
    "1. **Pruning**:\n",
    "   - Pruning techniques, such as cost complexity pruning, can be used to reduce the size of the tree and avoid overfitting.\n",
    "\n",
    "2. **Ensemble Methods**:\n",
    "   - Techniques like Random Forests and Gradient Boosting combine multiple decision trees to improve robustness, accuracy, and stability.\n",
    "\n",
    "3. **Regularization**:\n",
    "   - Implementing constraints such as limiting the maximum depth of the tree or the minimum number of samples required to split a node can help control overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question: How do random forests improve upon decision trees.\n",
    "#Ans:\n",
    "**Random Forests** enhance decision trees by:\n",
    "\n",
    "1. **Ensemble Learning**: Combines multiple trees to improve accuracy and robustness.\n",
    "2. **Reduces Overfitting**: Aggregates predictions to lower variance and generalize better.\n",
    "3. **Bagging**: Uses random subsets of data (with replacement) for training each tree.\n",
    "4. **Feature Randomness**: Each split considers a random subset of features, preventing any single feature from dominating.\n",
    "5. **Improved Accuracy**: Averaging predictions from diverse trees results in a more accurate model.\n",
    "6. **Feature Importance**: Provides measures of feature importance, aiding in feature selection.\n",
    "7. **Robustness**: Less sensitive to noise and outliers compared to individual decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Random Forest Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#code example\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Train a Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question:1 How does a random forest algorithm work.\n",
    "#Answer:\n",
    "### How Random Forest Algorithm Works\n",
    "\n",
    "1. **Data Sampling (Bagging)**:\n",
    "   - **Process**: Randomly sample subsets of the training data with replacement to create multiple bootstrap samples.\n",
    "   - **Purpose**: Each tree in the forest is trained on a different subset, enhancing diversity and reducing overfitting.\n",
    "\n",
    "2. **Building Decision Trees**:\n",
    "   - **Process**: For each bootstrap sample, build a decision tree. During training, each tree considers a random subset of features at each split.\n",
    "   - **Purpose**: Introduces randomness to prevent any single feature from dominating the model and to ensure that trees are diverse.\n",
    "\n",
    "3. **Tree Training**:\n",
    "   - **Process**: Each tree is trained to its full depth (or until a stopping criterion is met) on its bootstrap sample.\n",
    "   - **Purpose**: Trees are allowed to be deep and complex, but the aggregation of multiple trees prevents overfitting.\n",
    "\n",
    "4. **Prediction Aggregation**:\n",
    "   - **Classification**: Each tree in the forest makes a prediction (class label). The final prediction is the majority vote from all trees.\n",
    "   - **Regression**: Each tree predicts a value, and the final prediction is the average of all tree predictions.\n",
    "\n",
    "5. **Feature Importance**:\n",
    "   - **Process**: Calculate the importance of each feature based on how much it contributes to reducing impurity (e.g., Gini impurity, entropy) across all trees.\n",
    "   - **Purpose**: Provides insight into which features are most influential in making predictions.\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Random Forest Algorithm** works by:\n",
    "1. Creating multiple bootstrap samples from the training data.\n",
    "2. Training decision trees on these samples with random subsets of features.\n",
    "3. Aggregating predictions from all trees to improve accuracy and reduce overfitting.\n",
    "4. Measuring feature importance to understand the influence of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question: What is bootstrapping in the context of random forests.\n",
    "#Answer:\n",
    "### Bootstrapping in Random Forests\n",
    "\n",
    "1. **Definition**:\n",
    "   - **Bootstrapping**: A statistical method where multiple subsets of data are created by randomly sampling with replacement from the original dataset.\n",
    "\n",
    "2. **Process**:\n",
    "   - **Sampling**: Randomly select samples from the original dataset, allowing for the same sample to be selected more than once.\n",
    "   - **Subsets Creation**: Each bootstrap sample is the same size as the original dataset, but with some duplicates and some original data points left out.\n",
    "\n",
    "3. **Purpose in Random Forests**:\n",
    "   - **Diversity**: Ensures that each decision tree in the forest is trained on a different subset of the data, enhancing model diversity.\n",
    "   - **Reduction of Overfitting**: By averaging the predictions of multiple trees trained on different data samples, overfitting is reduced and generalization is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Bootstrap Sample: [ 7  4  8  5  7 10  3  7  8  5]\n"
     ]
    }
   ],
   "source": [
    "#code example\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Create a bootstrap sample\n",
    "bootstrap_sample = resample(data, replace=True, n_samples=len(data), random_state=42)\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Bootstrap Sample:\", bootstrap_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question: Explain the concept of feature importance in random forests.\n",
    "#Answer:\n",
    "**Feature Importance** in Random Forests refers to the process of identifying which features (variables) contribute the most to the model's predictions. It helps in understanding the significance of each feature and can be used for feature selection and interpretation. Here’s a concise explanation:\n",
    "\n",
    "### Concept of Feature Importance\n",
    "\n",
    "1. **Definition**:\n",
    "   - **Feature Importance**: Measures how much a feature contributes to reducing impurity (e.g., Gini impurity or entropy) in the decision trees of a Random Forest.\n",
    "\n",
    "2. **How It Works**:\n",
    "   - **Impurity Reduction**: During training, each decision tree in the Random Forest splits the data based on feature values. The reduction in impurity (e.g., decrease in Gini impurity or entropy) due to each split is tracked.\n",
    "   - **Aggregation**: The feature importance is calculated by aggregating the reduction in impurity across all trees in the forest. Features that lead to a higher reduction in impurity are deemed more important.\n",
    "\n",
    "3. **Calculation Methods**:\n",
    "   - **Mean Decrease in Impurity (MDI)**: Measures the total reduction in impurity brought by a feature, averaged over all trees.\n",
    "   - **Mean Decrease in Accuracy (MDA)**: Measures the decrease in model accuracy when the values of a feature are randomly shuffled, averaged over all trees.\n",
    "\n",
    "4. **Interpretation**:\n",
    "   - **High Importance**: Features with high importance scores are crucial for making accurate predictions and have a significant impact on the model.\n",
    "   - **Low Importance**: Features with low importance scores contribute less to the predictions and might be candidates for exclusion or further analysis.\n",
    "\n",
    "### Example\n",
    "\n",
    "In a Random Forest model for predicting house prices, feature importance might reveal:\n",
    "- **High Importance**: Features like \"Square Footage\" and \"Number of Bedrooms\" have high importance.\n",
    "- **Low Importance**: Features like \"House Color\" might have low importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What are the key hyperparameters of a random forest and how do they affect the model.\n",
    "# answer:\n",
    "**Key Hyperparameters of a Random Forest** and their effects on the model include:\n",
    "\n",
    "1. **Number of Trees (`n_estimators`)**:\n",
    "   - **Definition**: The total number of decision trees in the forest.\n",
    "   - **Effect**: More trees usually improve model performance by reducing variance and overfitting, but increases computation time. However, beyond a certain point, adding more trees provides diminishing returns.\n",
    "\n",
    "2. **Maximum Depth (`max_depth`)**:\n",
    "   - **Definition**: The maximum number of levels (or layers) in each decision tree.\n",
    "   - **Effect**: Limits how deep each tree can grow. Shallower trees might underfit (too simple), while deeper trees can overfit (too complex). Proper tuning helps balance bias and variance.\n",
    "\n",
    "3. **Minimum Samples per Leaf (`min_samples_leaf`)**:\n",
    "   - **Definition**: The minimum number of samples required to be at a leaf node.\n",
    "   - **Effect**: Controls tree growth. A higher value prevents the tree from growing too complex and helps in generalization, reducing overfitting.\n",
    "\n",
    "4. **Minimum Samples per Split (`min_samples_split`)**:\n",
    "   - **Definition**: The minimum number of samples required to split an internal node.\n",
    "   - **Effect**: Larger values prevent the model from creating splits that might overfit the training data. It controls the granularity of the splits.\n",
    "\n",
    "5. **Maximum Features (`max_features`)**:\n",
    "   - **Definition**: The maximum number of features considered when looking for the best split.\n",
    "   - **Effect**: Limits the number of features each tree can use to make splits. Smaller values introduce more randomness and reduce correlation between trees, improving generalization.\n",
    "\n",
    "6. **Bootstrap (`bootstrap`)**:\n",
    "   - **Definition**: Whether bootstrap samples are used when building trees.\n",
    "   - **Effect**: If `True`, each tree is trained on a random subset of the data (with replacement). If `False`, the whole dataset is used for each tree. Bootstrapping helps in reducing variance and overfitting.\n",
    "\n",
    "7. **Criterion (`criterion`)**:\n",
    "   - **Definition**: The function used to measure the quality of a split.\n",
    "   - **Effect**: Common options are \"gini\" for Gini impurity and \"entropy\" for information gain. The choice affects how splits are evaluated but typically doesn’t significantly impact performance.\n",
    "\n",
    "8. **Random State (`random_state`)**:\n",
    "   - **Definition**: Seed for the random number generator.\n",
    "   - **Effect**: Ensures reproducibility of the results by controlling the randomness in sampling and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest classifier with key hyperparameters\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    max_depth=None,         # Maximum depth of each tree\n",
    "    min_samples_split=2,    # Minimum samples required to split a node\n",
    "    min_samples_leaf=1,     # Minimum samples required at a leaf node\n",
    "    max_features='auto',    # Number of features to consider for the best split\n",
    "    bootstrap=True,         # Whether to use bootstrap samples\n",
    "    random_state=42,        # Seed for reproducibility\n",
    "    criterion='gini'        # Function to measure the quality of a split\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Describe the logistic regression model and its assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer:\n",
    "### Logistic Regression Model\n",
    "\n",
    "**Logistic Regression** is a statistical method used for binary classification problems. It estimates the probability of a binary outcome based on one or more predictor variables.\n",
    "\n",
    "#### Model Description\n",
    "\n",
    "1. **Function**:\n",
    "   - Logistic Regression predicts the probability that a given input belongs to a particular class.\n",
    "   - The output is a probability value between 0 and 1, which is then used to classify the data point.\n",
    "\n",
    "2. **Decision Boundary**:\n",
    "   - A threshold (usually 0.5) is applied to the predicted probability to classify the input into one of the two classes.\n",
    "\n",
    "#### Assumptions of Logistic Regression\n",
    "\n",
    "1. **Binary Outcome**:\n",
    "   - The dependent variable must be binary (i.e., it can take only two possible outcomes).\n",
    "\n",
    "2. **Independence of Errors**:\n",
    "   - The observations should be independent of each other. This means that the residuals (errors) of the predictions should not be correlated.\n",
    "\n",
    "3. **No Multicollinearity**:\n",
    "   - The predictor variables should not be highly correlated with each other. High multicollinearity can make it difficult to estimate the coefficients accurately.\n",
    "\n",
    "4. **Large Sample Size**:\n",
    "   - Logistic regression requires a reasonably large sample size to provide reliable estimates and stable results.\n",
    "\n",
    "5. **Homoscedasticity** (for some cases):\n",
    "   - While not a strict assumption for logistic regression (which models probabilities rather than continuous outcomes), homoscedasticity in predictor variables can improve the robustness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:  How does logistic regression handle binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ans:\n",
    "### Handling Binary Classification with Logistic Regression\n",
    "\n",
    "**Logistic Regression** addresses binary classification problems by modeling the probability that a given input belongs to one of two classes. Here’s how it works:\n",
    "\n",
    "1. **Probability Estimation**:\n",
    "   - **Model**: Logistic regression uses the logistic (sigmoid) function to estimate the probability of the positive class.\n",
    "   - This function maps any real-valued input into a probability between 0 and 1.\n",
    "\n",
    "2. **Logit Transformation**:\n",
    "   - **Logit Function**: Converts the probability into a log-odds scale:\n",
    "   - The logit is a linear combination of the input features.\n",
    "\n",
    "3. **Decision Boundary**:\n",
    "   - **Threshold**: Applies a decision threshold (typically 0.5) to classify the outcome.\n",
    "   - If the predicted probability is greater than or equal to 0.5, classify as positive (1); otherwise, classify as negative (0).\n",
    "\n",
    "4. **Training the Model**:\n",
    "   - **Objective**: Estimates the model coefficients (\\(\\beta\\)) that maximize the likelihood of the observed data.\n",
    "   - **Method**: Uses optimization techniques (like gradient descent) to find the best-fitting coefficients.\n",
    "\n",
    "5. **Prediction**:\n",
    "   - **Probability Output**: Provides a probability score indicating the likelihood of the input belonging to the positive class.\n",
    "   - **Classification**: Converts this probability into a binary class label based on the decision threshold.\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Logistic Regression** handles binary classification by:\n",
    "- Estimating the probability of the positive class using the logistic function.\n",
    "- Using the logit transformation to model the relationship between features and the probability.\n",
    "- Applying a decision threshold to classify outcomes.\n",
    "- Training the model to optimize coefficients that best fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question:1 What is the sigmoid function and how is it used in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "The **sigmoid function**, also known as the logistic function, is a key component of logistic regression, particularly for binary classification problems. Here’s a concise explanation of what it is and how it’s used:\n",
    "\n",
    "### Definition of the Sigmoid Function\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "sigma(z) = 1/(1 + e^{-z}) \n",
    "\n",
    "where ( z ) is the input to the function, which is typically a linear combination of the features and their corresponding weights in the model.\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "1. **Range**: The sigmoid function maps any real-valued number ( z ) into a value between 0 and 1. This makes it ideal for modeling probabilities.\n",
    "2. **Shape**: It produces an S-shaped curve (hence \"sigmoid\") that smoothly transitions between 0 and 1.\n",
    "\n",
    "### Usage in Logistic Regression\n",
    "\n",
    "1. **Probability Estimation**:\n",
    "   - In logistic regression, the sigmoid function is used to estimate the probability of a binary outcome. The function takes a linear combination of the input features (and their weights) and converts it into a probability.\n",
    "   \n",
    "2. **Decision Boundary**:\n",
    "   - The sigmoid function outputs a probability between 0 and 1. In logistic regression, a threshold (usually 0.5) is applied to this probability to classify the input into one of the two classes. If the probability is greater than or equal to 0.5, the model predicts the positive class; otherwise, it predicts the negative class.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - The output of the sigmoid function represents the probability of the input belonging to the positive class. For example, if the sigmoid function outputs 0.8, it indicates an 80% probability that the input belongs to the positive class.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a logistic regression model used to predict whether a customer will purchase a product (yes/no). The model calculates a score (linear combination of features) and applies the sigmoid function to this score. If the sigmoid function outputs 0.7, the model predicts that there is a 70% probability that the customer will purchase the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: Explain the concept of the cost function in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ans:\n",
    "The **cost function** in logistic regression is a measure of how well the model's predictions match the actual outcomes.\n",
    "Here's a concise explanation of the concept:\n",
    "\n",
    "### Concept of the Cost Function\n",
    "\n",
    "1. **Purpose**:\n",
    "   - The cost function aims to evaluate the performance of the logistic regression model by calculating the difference between predicted probabilities and actual binary outcomes.\n",
    "\n",
    "2. **Interpretation**:\n",
    "   - **Log Loss** calculates the average of the negative log likelihood of the predictions. It penalizes incorrect predictions more severely as the predicted probability deviates further from the actual label.\n",
    "   - When predictions are very close to the actual outcomes, the cost is low. When predictions are far from the actual outcomes, the cost is high.\n",
    "\n",
    "3. **Optimization**:\n",
    "   - The goal of training is to **minimize** the cost function. Optimization algorithms (like gradient descent) adjust the model parameters (\\(\\theta\\)) to reduce the cost and improve the accuracy of the model.\n",
    "\n",
    "4. **Example**:\n",
    "   - Suppose you have a logistic regression model predicting whether an email is spam or not. If the model predicts a 0.8 probability for an email being spam and the actual label is 1 (spam), the log loss will reflect the cost of this prediction. If the predicted probability is close to the actual label, the cost will be lower; if it’s far, the cost will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question :How can logistic regression be extended to handle multiclass classification.\n",
    "# answer:\n",
    "\n",
    "Logistic regression can be extended to handle multiclass classification problems using two primary approaches: **One-vs-Rest (OvR)** and **Softmax Regression** (also known as **Multinomial Logistic Regression**). Here’s a concise explanation of each method:\n",
    "\n",
    "### 1. One-vs-Rest (OvR) Approach\n",
    "\n",
    "**One-vs-Rest (OvR)**, also known as **One-vs-All (OvA)**, involves decomposing a multiclass classification problem into multiple binary classification problems.\n",
    "\n",
    "- **Method**:\n",
    "  - For a problem with \\( K \\) classes, OvR trains \\( K \\) separate binary classifiers.\n",
    "  - Each classifier is trained to distinguish one class from all other classes.\n",
    "  - For each sample, the classifier with the highest probability (or confidence) determines the predicted class.\n",
    "\n",
    "- **Example**:\n",
    "  - If you want to classify emails into three categories: Spam, Promotions, and Updates, OvR will create three separate classifiers:\n",
    "    - Spam vs. (Promotions + Updates)\n",
    "    - Promotions vs. (Spam + Updates)\n",
    "    - Updates vs. (Spam + Promotions)\n",
    "  - During prediction, each classifier provides a probability, and the class with the highest probability is chosen as the final prediction.\n",
    "\n",
    "### 2. Softmax Regression (Multinomial Logistic Regression)\n",
    "\n",
    "**Softmax Regression** extends logistic regression to directly handle multiple classes by generalizing the logistic function to multiple categories.\n",
    "\n",
    "- **Method**:\n",
    "  - The model computes a probability distribution over \\( K \\) classes using the **Softmax function**:\n",
    "  - The Softmax function outputs probabilities for each class, and the class with the highest probability is selected.\n",
    "\n",
    "- **Example**:\n",
    "  - For the email classification problem with classes Spam, Promotions, and Updates, Softmax Regression will output probabilities for each class:\n",
    "  \n",
    "    >P(Spam |X)\n",
    "    >P(Promotions |X)\n",
    "    >P(Updates |X)\n",
    "\n",
    "  - The class with the highest probability is chosen as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvR Accuracy: 0.9666666666666667\n",
      "Softmax Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#code example\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train logistic regression model with OvR\n",
    "log_reg_ovr = LogisticRegression(multi_class='ovr')\n",
    "log_reg_ovr.fit(X_train, y_train)\n",
    "y_pred_ovr = log_reg_ovr.predict(X_test)\n",
    "print(\"OvR Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n",
    "\n",
    "# Initialize and train logistic regression model with softmax regression\n",
    "log_reg_softmax = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "log_reg_softmax.fit(X_train, y_train)\n",
    "y_pred_softmax = log_reg_softmax.predict(X_test)\n",
    "print(\"Softmax Accuracy:\", accuracy_score(y_test, y_pred_softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: What is XGBoost and how does it differ from other boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "**XGBoost** (Extreme Gradient Boosting) is an optimized machine learning algorithm designed for efficiency and performance. It’s widely used in classification and regression problems and is known for its speed and accuracy. \n",
    "\n",
    "### What is XGBoost?\n",
    "\n",
    "**XGBoost** is an implementation of gradient boosting that enhances performance and efficiency. It builds an ensemble of decision trees, where each tree corrects the errors of its predecessors.\n",
    "\n",
    "- **Boosting Mechanism**: Combines multiple weak learners (usually decision trees) to create a strong learner. Each tree is built based on the errors of the previous trees, and the final model is a weighted sum of all the trees.\n",
    "- **Objective Function**: Minimizes a loss function using gradient descent. It optimizes both the prediction accuracy and model complexity.\n",
    "\n",
    "### Key Features of XGBoost\n",
    "\n",
    "1. **Regularization**:\n",
    "   - **L1 (Lasso) and L2 (Ridge)** regularization to prevent overfitting.\n",
    "   - Helps in feature selection and improves model generalization.\n",
    "\n",
    "2. **Parallelization**:\n",
    "   - Efficiently utilizes multiple cores during training, speeding up the process significantly compared to traditional gradient boosting methods.\n",
    "\n",
    "3. **Tree Pruning**:\n",
    "   - Uses a depth-first approach to grow trees and prunes them by splitting nodes until a maximum depth or complexity is reached.\n",
    "\n",
    "4. **Handling Missing Values**:\n",
    "   - Automatically handles missing values during training without the need for imputation.\n",
    "\n",
    "5. **Flexibility**:\n",
    "   - Supports various objective functions (e.g., binary classification, regression, ranking) and evaluation metrics.\n",
    "\n",
    "6. **Cross-Validation**:\n",
    "   - Built-in support for cross-validation to tune hyperparameters and prevent overfitting.\n",
    "\n",
    "### Differences from Other Boosting Algorithms\n",
    "\n",
    "1. **Gradient Boosting Machines (GBM)**:\n",
    "   - **XGBoost** is faster due to its parallelized implementation and advanced optimization techniques.\n",
    "   - **XGBoost** includes regularization, while traditional **GBM** does not.\n",
    "\n",
    "2. **AdaBoost**:\n",
    "   - **AdaBoost** focuses on correcting misclassified samples by adjusting their weights, while **XGBoost** uses gradient descent to minimize the loss function iteratively.\n",
    "   - **XGBoost** can handle more complex models with regularization, while **AdaBoost** typically uses simpler base models.\n",
    "\n",
    "3. **LightGBM**:\n",
    "   - **LightGBM** (Light Gradient Boosting Machine) is another gradient boosting framework optimized for speed and efficiency. It differs from **XGBoost** in its use of histogram-based algorithms and categorical feature handling.\n",
    "   - **XGBoost** often performs better with smaller datasets, while **LightGBM** can handle larger datasets more efficiently.\n",
    "\n",
    "4. **CatBoost**:\n",
    "   - **CatBoost** (Categorical Boosting) is specifically designed to handle categorical features without extensive preprocessing. It differs from **XGBoost** in its approach to categorical variables and its symmetric tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: Explain the concept of boosting in the context of ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "**Boosting** is a powerful ensemble learning technique that aims to improve the performance of machine learning models by combining multiple weak learners to form a strong learner. Here’s a concise explanation of how boosting works in the context of ensemble learning:\n",
    "\n",
    "### Concept of Boosting\n",
    "\n",
    "1. **Objective**:\n",
    "   - Boosting focuses on creating a strong predictive model by sequentially combining a series of weak learners. A weak learner is a model that performs slightly better than random guessing (e.g., a shallow decision tree).\n",
    "\n",
    "2. **Sequential Learning**:\n",
    "   - Boosting builds models in a sequence where each new model attempts to correct the errors of its predecessors. Each weak learner is trained to focus on the mistakes made by the previous models.\n",
    "\n",
    "3. **Error Correction**:\n",
    "   - **Weight Adjustment**: Boosting algorithms assign higher weights to the samples that were misclassified by previous models. This way, the subsequent models pay more attention to these difficult cases and try to correct their errors.\n",
    "   - **Example**: In a classification problem, if the previous models misclassified certain samples, the next model will give these samples more importance, thereby improving the overall accuracy.\n",
    "\n",
    "4. **Model Combination**:\n",
    "   - After training each weak learner, their predictions are combined to form the final model. The combination is usually done through weighted voting (for classification) or weighted averaging (for regression).\n",
    "   - **Example**: If you have three weak models that predict whether an email is spam or not, their predictions are combined based on their weights to make the final decision.\n",
    "\n",
    "5. **Key Boosting Algorithms**:\n",
    "   - **AdaBoost**: Focuses on adjusting the weights of misclassified samples and combines weak classifiers into a strong one.\n",
    "   - **Gradient Boosting**: Builds models sequentially where each model corrects the residual errors of the previous ones using gradient descent.\n",
    "   - **XGBoost**: An optimized version of gradient boosting that enhances performance with additional features like regularization and parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: How does XGBoost handle missing values.\n",
    "# Ans:\n",
    "\n",
    "<<XGBoost handles missing values automatically during training. Here's a brief overview and relevant commands:\n",
    "\n",
    "<<Handling Missing Values in XGBoost>>\n",
    "1. Automatic Handling:\n",
    "<XGBoost detects and handles missing values without explicit imputation.\n",
    "\n",
    "2. Tree Construction:\n",
    "<During tree construction, XGBoost learns the best direction (left or right) for missing values to optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy with Missing Values: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#Example code\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Introduce missing values\n",
    "np.random.seed(42)\n",
    "missing_mask = np.random.rand(*X.shape) < 0.1\n",
    "X[missing_mask] = np.nan\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, missing=np.nan)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(\"XGBoost Accuracy with Missing Values:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question: 1 What are the key hyperparameters in XGBoost and how do they affect model performance.\n",
    "# Answer:\n",
    "Here’s a concise overview of key XGBoost hyperparameters and their effects on model performance:\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **Learning Rate (`eta`)**\n",
    "   - **Effect**: Controls step size; lower values make the model more robust but require more trees.\n",
    "\n",
    "2. **Number of Trees (`n_estimators`)**\n",
    "   - **Effect**: Number of boosting rounds; more trees can improve performance but increase computational cost.\n",
    "\n",
    "3. **Maximum Depth (`max_depth`)**\n",
    "   - **Effect**: Controls tree depth; deeper trees capture more complexity but risk overfitting.\n",
    "\n",
    "4. **Minimum Child Weight (`min_child_weight`)**\n",
    "   - **Effect**: Minimum sum of weights for a child node; higher values prevent overfitting.\n",
    "\n",
    "5. **Subsample**\n",
    "   - **Effect**: Fraction of samples used for training; lower values add randomness and prevent overfitting.\n",
    "\n",
    "6. **Colsample Bytree (`colsample_bytree`)**\n",
    "   - **Effect**: Fraction of features used for each tree; helps reduce overfitting by introducing feature randomness.\n",
    "\n",
    "7. **Gamma**\n",
    "   - **Effect**: Minimum loss reduction required for a split; higher values reduce overfitting.\n",
    "\n",
    "8. **Lambda (`reg_lambda`)**\n",
    "   - **Effect**: L2 regularization term; penalizes large coefficients to reduce overfitting.\n",
    "\n",
    "9. **Alpha (`reg_alpha`)**\n",
    "   - **Effect**: L1 regularization term; encourages sparsity and feature selection.\n",
    "\n",
    "10. **Scale Pos Weight**\n",
    "    - **Effect**: Balances the weight of positive and negative samples; useful for handling class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Initialize and train XGBoost model with hyperparameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    reg_lambda=1,\n",
    "    reg_alpha=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: 1 Describe the process of gradient boosting in XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "**Gradient Boosting** in XGBoost involves sequentially improving a model by correcting the errors of its predecessors. Here’s a concise overview:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with a base model, typically a simple prediction (e.g., mean for regression or a constant value for classification).\n",
    "\n",
    "2. **Compute Residuals**:\n",
    "   - Calculate residuals (errors) from the base model’s predictions by comparing them with actual target values.\n",
    "\n",
    "3. **Train New Tree**:\n",
    "   - Fit a new decision tree to the residuals. This tree aims to capture and correct the errors made by the base model.\n",
    "\n",
    "4. **Update Model**:\n",
    "   - Add the predictions from the new tree to the base model, scaled by a learning rate to control the contribution of each tree.\n",
    "\n",
    "5. **Repeat**:\n",
    "   - Iterate the process: compute new residuals based on the updated model, train another tree, and update the model. Continue for a specified number of iterations or until performance plateaus.\n",
    "\n",
    "6. **Combine Trees**:\n",
    "   - The final model combines predictions from all trees. Each tree’s contribution is weighted by the learning rate to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: What are the advantages and disadvantages of using XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "**XGBoost** offers several advantages and some disadvantages. Here’s a summary:\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **High Performance**:\n",
    "   - **Speed**: Optimized for fast training and prediction. It efficiently utilizes hardware resources.\n",
    "   - **Accuracy**: Often achieves superior accuracy compared to other models due to its ability to handle complex patterns.\n",
    "\n",
    "2. **Flexibility**:\n",
    "   - **Support for Various Objectives**: Handles regression, classification, ranking, and user-defined prediction tasks.\n",
    "   - **Customizable**: Allows for custom objective functions and evaluation metrics.\n",
    "\n",
    "3. **Regularization**:\n",
    "   - **Built-in Regularization**: Includes L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting and improve model generalization.\n",
    "\n",
    "4. **Feature Importance**:\n",
    "   - **Feature Selection**: Provides insights into feature importance, aiding in understanding and selecting relevant features.\n",
    "\n",
    "5. **Handling Missing Values**:\n",
    "   - **Automatic**: Handles missing values internally without the need for explicit imputation.\n",
    "\n",
    "6. **Cross-Validation**:\n",
    "   - **Built-in**: Supports cross-validation during training to tune hyperparameters and avoid overfitting.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Complexity**:\n",
    "   - **Hyperparameter Tuning**: Requires careful tuning of hyperparameters, which can be time-consuming and complex.\n",
    "   - **Model Interpretability**: More complex models can be harder to interpret compared to simpler models.\n",
    "\n",
    "2. **Resource Intensive**:\n",
    "   - **Memory Usage**: Can be memory-intensive, especially with large datasets and deep trees.\n",
    "   - **Computational Cost**: High performance comes with increased computational demands.\n",
    "\n",
    "3. **Overfitting**:\n",
    "   - **Risk of Overfitting**: Despite regularization, it can still overfit if not properly tuned, particularly with a high number of trees or deep trees.\n",
    "\n",
    "4. **Learning Curve**:\n",
    "   - **Steeper Learning Curve**: Requires a good understanding of its parameters and features to fully leverage its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
